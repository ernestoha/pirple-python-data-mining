{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark HomeWork6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pirple/Data-Mining-With-Python/blob/master/Part%204/Section_3.1_3.2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libs and pyspark init \n",
    "import pyspark.sql.functions as F\n",
    "import pyspark as ps\n",
    "from pyspark import SQLContext  \n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "    .master('local[2]') \\\n",
    "    .appName('spark-ml') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = 'Data/austin_weather.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc) # crear var para ejecutar sentencias sql df = sqlContext.sql(\"SELECT * FROM table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------+-------------+------------+------------+-------------------+------------------+------------------+--------------------------+-------------------------+-------------------------+-------------------+------------------+------------------+-----------+----------+-----------+----------------------+-------------------+\n",
      "|      Date|TempHighF|TempAvgF|TempLowF|DewPointHighF|DewPointAvgF|DewPointLowF|HumidityHighPercent|HumidityAvgPercent|HumidityLowPercent|SeaLevelPressureHighInches|SeaLevelPressureAvgInches|SeaLevelPressureLowInches|VisibilityHighMiles|VisibilityAvgMiles|VisibilityLowMiles|WindHighMPH|WindAvgMPH|WindGustMPH|PrecipitationSumInches|             Events|\n",
      "+----------+---------+--------+--------+-------------+------------+------------+-------------------+------------------+------------------+--------------------------+-------------------------+-------------------------+-------------------+------------------+------------------+-----------+----------+-----------+----------------------+-------------------+\n",
      "|2013-12-21|       74|      60|      45|           67|          49|          43|                 93|                75|                57|                     29.86|                    29.68|                    29.59|                 10|                 7|                 2|         20|         4|         31|                  0.46|Rain , Thunderstorm|\n",
      "|2013-12-22|       56|      48|      39|           43|          36|          28|                 93|                68|                43|                     30.41|                    30.13|                    29.87|                 10|                10|                 5|         16|         6|         25|                     0|                   |\n",
      "|2013-12-23|       58|      45|      32|           31|          27|          23|                 76|                52|                27|                     30.56|                    30.49|                    30.41|                 10|                10|                10|          8|         3|         12|                     0|                   |\n",
      "|2013-12-24|       61|      46|      31|           36|          28|          21|                 89|                56|                22|                     30.56|                    30.45|                     30.3|                 10|                10|                 7|         12|         4|         20|                     0|                   |\n",
      "|2013-12-25|       58|      50|      41|           44|          40|          36|                 86|                71|                56|                     30.41|                    30.33|                    30.27|                 10|                10|                 7|         10|         2|         16|                     T|                   |\n",
      "+----------+---------+--------+--------+-------------+------------+------------+-------------------+------------------+------------------+--------------------------+-------------------------+-------------------------+-------------------+------------------+------------------+-----------+----------+-----------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV Spark\n",
    "df_aapl = sqlContext.read.csv(csv_data,\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5) #df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-bf37e573b043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_aapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_aapl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_aapl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_aapl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1401\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1402\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "df_aapl = df_aapl.withColumn(\"m\", df_aapl.Date) #new Column\n",
    "df_aapl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "df_aapl.createOrReplaceTempView(\"df_aapl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------+\n",
      "|date_add(to_date(a.`Date`), 1)|TempHighF|\n",
      "+------------------------------+---------+\n",
      "|                    2013-12-22|       74|\n",
      "|                    2013-12-23|       56|\n",
      "|                    2013-12-24|       58|\n",
      "|                    2013-12-25|       61|\n",
      "|                    2013-12-26|       58|\n",
      "|                    2013-12-27|       57|\n",
      "|                    2013-12-28|       60|\n",
      "|                    2013-12-29|       62|\n",
      "|                    2013-12-30|       64|\n",
      "|                    2013-12-31|       44|\n",
      "|                    2014-01-01|       55|\n",
      "|                    2014-01-02|       69|\n",
      "|                    2014-01-03|       55|\n",
      "|                    2014-01-04|       58|\n",
      "|                    2014-01-05|       71|\n",
      "|                    2014-01-06|       59|\n",
      "|                    2014-01-07|       36|\n",
      "|                    2014-01-08|       48|\n",
      "|                    2014-01-09|       53|\n",
      "|                    2014-01-10|       70|\n",
      "+------------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data01 = sqlContext.sql(\"SELECT to_date(a.Date)+1, a.TempHighF FROM df_aapl a INNER JOIN df_aapl b ON a.Date=b.Date \")\n",
    "data01.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      Date|TempHighF|\n",
      "+----------+---------+\n",
      "|2013-12-31|       55|\n",
      "|2014-01-01|       69|\n",
      "|2014-01-04|       71|\n",
      "|2014-01-07|       48|\n",
      "|2014-01-09|       70|\n",
      "|2014-01-20|       82|\n",
      "|2014-01-25|       70|\n",
      "|2014-01-29|       49|\n",
      "|2014-01-30|       65|\n",
      "|2014-01-31|       80|\n",
      "+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where = \"WHERE 1=1\"\n",
    "where = \"WHERE to_date(a.Date)+1 = to_date(b.Date) AND b.TempHighF-a.TempHighF>=10\"\n",
    "# tempHigh = sqlContext.sql(\"SELECT to_date(a.Date)+1, a.TempHighF FROM df_aapl a INNER JOIN df_aapl b ON a.Date=b.Date \" + where)\n",
    "tempHigh = sqlContext.sql(\"SELECT b.Date, b.TempHighF FROM df_aapl a, df_aapl b \" + where)\n",
    "tempHigh.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|TempHighF|\n",
      "+---------+\n",
      "|       74|\n",
      "|       56|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempHigh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempHigh.write.csv(\"Data/Homework6OutPut\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
